---
title: "Project 2"
author: "Avy Harvey"
date: "6/27/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(glmnet)  # For LASSO-regularized logistic regression
library(doParallel)  # For parallelizing model building
knitr::opts_chunk$set(echo = TRUE)
```

## Goal

~~The goal is to create models for predicting the shares variable from the dataset. You will create two models: a
linear regression model and a non-linear model (each of your choice). You will use the parameter functionality
of markdown to automatically generate an analysis report for each weekday_is_* variable (so you’ll end up
with seven total outputted documents).

At first, consider just using the ‘Monday’ data. Once you have all of the below steps done for that data, then
automate it to work with any chosen day of the week data.~~

## Introduction

The purpose of this project is to build models to predict the number of social media shares that an online news article will receive. Specifically, I will train several logistic regression models and random forest models, use repeated 10-fold cross-validation to select a single candidate model for each type of algorithm, then compare those candidate models on the holdout test data set. This process will be repeated for every day of the week on which an article can be published (i.e., Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday).

The data used in this project is the [Online News Popularity](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) data set available from the UCI Machine Learning Repository. All of the news articles in referenced in this data set were published by [Mashable](https://www.mashable.com), and can be referenced using the `url` variable. The data dictionary is available on the UCI website for the data set linked above.

## Data

~~You should briefly describe the data and the variables you have to work with (no need to discuss all of them, just the ones you want to use).~~

The data set contains 58 predictor variables and one target variable (`shares`). The predictor variables are either numeric or binary and represent various statistics associated with the news articles.

As mentioned before, I will be creating a different model for each day of the week, as denoted by the `weekday_is_*` indicator variables. Therefore, these variables (along with `is_weekend`) will not be included in my model-building process.

The other predictor variables attempt to measure a number of things. Some metadata about the article is available, such as number of links, images, and videos. Additionally, the channel (or segment) that the article falls under is also available as a set of indicator variables. These include channels such as lifestyle, entertainment, and technology.

Most of the remaining predictor variables are derived using natural language processing techniques. These include variables that tokenize the text and provide counts, such as number of tokens in the title, article content, stop words, and unique tokens. There is another set of indicator variables that is created from a form of unsupervised clustering in natural language processing, called Latent Dirichlet allocation (LDA). There are also variables related to article sentiment and polarity. These variables describe the overall feeling that an article's text provides, as well as describing how polarizing the language in the title is.

Because there are so many variables, I will need to do some variable selection to reduce the dimensionality of the data set, which will hopefully result in simpler models that are able to better generalize to the data.

First, I'll read in the data, filter it for the weekday for which I will build a model, and remove the columns that I won't be using:

```{r}
df <- read_csv("data/OnlineNewsPopularity.csv") %>%
  filter(weekday_is_monday == 1) %>%
  select(-starts_with("weekday_is_"), -is_weekend, -url, -timedelta)
```

~~You should randomly sample from (say using sample()) the (Monday) data in order to form a training (70% of the data) and test set (30% of the data). You should set the seed to make your work reproducible.~~

## Summarizations

For news articles to be shared on social media, I hypothesize that the reader must have been interested in it in some way. There are several reasons why a person might find an article interesting.

### Shares

**Q: What does the distribution of the `shares` variable look like?**

First, I'll calculate a numeric summary of `shares`, which should give me an idea of how spread out the data is (including outliers):

```{r}
summary(df$shares)

# Standard deviation
sd(df$shares)
```

In this graph, I expect the distribution to be right-skewed due to viral news articles that get an extremely high number of shares. To keep the distribution visible, I filter out the outliers before plotting.

```{r}
df %>%
  filter(shares < 3 * sd(shares)) %>%
  ggplot(aes(x = shares)) +
  geom_histogram(binwidth = 1000) +
  labs(title = "Histogram of Shares", x = "Number of Shares", y = "Number of Articles")
```

### Channel/Topic

**Q: How many articles are published in a given channel or topic, and how many shares do they get?**

This graph displays the number of articles that were published under a particular topic. I noticed that there are several articles that don't match one of the indicator variables, so I created another category called "Other".

```{r}
df %>%
  select(starts_with("data_channel_is_")) %>%
  rename(Lifestyle = data_channel_is_lifestyle,
         Entertainment = data_channel_is_entertainment,
         Business = data_channel_is_bus,
         `Social Media` = data_channel_is_socmed,
         Technology = data_channel_is_tech,
         World = data_channel_is_world) %>%
  mutate(Other = !(Lifestyle | Entertainment | Business | `Social Media` | Technology | World)) %>%
  pivot_longer(everything(), names_to="Channel") %>%
  filter(value == 1) %>%
  ggplot(aes(x = Channel, group = Channel)) +
  geom_bar(aes(fill = Channel), show.legend = FALSE) +
  labs(title = "Number of Articles by Channel", y = "Number of Articles")
```

Next, I want to explore the distribution of shares for each of those channels. This can help me understand which channels may be associated with a higher number of shares. I'll exclude outliers in this graph so that the boxes are visible.

```{r}
df %>%
  select(starts_with("data_channel_is_"), shares) %>%
  rename(Lifestyle = data_channel_is_lifestyle,
         Entertainment = data_channel_is_entertainment,
         Business = data_channel_is_bus,
         `Social Media` = data_channel_is_socmed,
         Technology = data_channel_is_tech,
         World = data_channel_is_world) %>%
  mutate(Other = !(Lifestyle | Entertainment | Business | `Social Media` | Technology | World)) %>%
  pivot_longer(-shares, names_to="Channel") %>%
  filter(value == 1 & shares < 3 * sd(df$shares)) %>%  # Exclude outliers
  ggplot(aes(x = Channel, y = shares)) +
  geom_boxplot(aes(fill = Channel)) +
  labs(title = "Number of Shares by Channel", y = "Number of Shares")
```

### Sentiment

**Q: How does title subjectivity affect number of shares?**

A title's subjectivity tries to measure much a title sounds like an opinion versus being factual. I hypothesize that titles that sound factual and confident are more likely to grab a readers attention than an opinionated title.

First, let's look at a quick summary of the variable before visualizing a comparison:

```{r}
summary(df$title_subjectivity)
```

```{r}
df %>%
  filter(shares < 3 * sd(shares)) %>%  # Exclude outliers
  ggplot(aes(x = title_subjectivity, y = shares)) +
  geom_point(alpha = 0.25)
  labs(title = "Shares vs. Title Subjectivity", x = "Title Subjectivity", y = "Number of Shares")
```

**Q: How does text sentiment polarity affect number of shares?**

Polarity tries to measure how negative or positive a document sounds. I suspect the extremes may affect how likely a person is to share a news article.

Again, we'll first look at a numeric summary before visualizing a comparison:

```{r}
summary(df$global_sentiment_polarity)
```

```{r}
df %>%
  filter(shares < 3 * sd(shares)) %>%  # Exclude outliers
  ggplot(aes(x = global_sentiment_polarity, y = shares)) +
  geom_point(alpha = 0.1) +
  labs(title = "Shares vs. Text Sentiment Polarity", x = "Text Sentiment Polarity", y = "Number of Shares")
```

~~You should produce some basic (but meaningful) summary statistics about the training data you are working
with. The general things that the plots describe should be explained but, since we are going to automate
things, there is no need to try and explain particular trends in the plots you see (unless you want to try and
automate that too!).~~

### LDA

**Q: How does LDA closeness affect number of shares?**

LDA closeness measures how close a document is to an unlabeled topic. This can be thought of as closeness to a centroid in k-means clustering, but for natural language processing. I want to see if particular LDA topics affect how often an article is shared.

```{r}
df %>%
  filter(shares < 3 * sd(shares)) %>%  # Exclude outliers
  mutate(n = row_number()) %>%
  pivot_longer(starts_with("LDA"), names_to = "LDA") %>%
  group_by(n) %>%
  # Code referenced from https://stackoverflow.com/a/29657877
  slice(which.max(value)) %>%  # Don't drop other columns, but log max LDA
  ggplot(aes(x = LDA, y = shares)) +
  geom_boxplot(aes(fill = LDA), show.legend = FALSE) +
  labs(title = "Shares by LDA Topic", x = "LDA Topic", y = "Number of Shares")
```

## Modeling

I'm going to turn this problem into a binary classification problem by splitting the `shares` variable into two groups: $shares \lt 1400$ and $shares \ge 1400$.

```{r}
df_bin <- df %>%
  mutate(sharesBin = as_factor(ifelse(shares < 1400, 0, 1))) %>%
  select(-shares)
```


Before I start training my models, I'll split the data into a train and test set using random sampling:

```{r}
# Set seed for reproducibility
set.seed(1)

# Sample training indices
train_index <- createDataPartition(df_bin$sharesBin, p = .7)[[1]]

# Create train and test sets
df_train <- df_bin[train_index,]
df_test <- df_bin[-train_index,]
```

~~Once you have your training data set, we are ready to fit some models.
You should fit two types of models to predict the shares. One model should be an ensemble model (bagged
trees, random forests, or boosted trees) and one should be a linear regression model (or collection of them
that you’ll choose from).~~

### Random Forest

The ensemble model that I will try is a random forest. I will preprocess the predictor variables with centering and scaling. I will use repeated 10-fold cross validation to select the best value of `mtry` (the parameter which controls how many predictors are randomly sampled at each split), among several distinct values of `mtry`. That model will be the selected candidate model for Random Forest.

```{r}
# Process training in parallel
# Referenced caret vignette at https://topepo.github.io/caret/parallel-processing.html
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

# Set seed for reproducibility
set.seed(1)

# Do repeated cross validation 3 times
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

# Build the random forest model using 10 different values of mtry
random_forest_fit <- train(sharesBin ~ ., data = df_train, method = "rf",
 trControl=train_control,
 preProcess = c("center", "scale"),
 tuneGrid = expand.grid(.mtry = c(2:8, 15, 30, 45)))

# Turn off the cluster for parallel processing
stopCluster(cl)

# Show the results
random_forest_fit
```

### Logistic Regression

The linear model that I will try is logistic regression since the target is binary. I will preprocess the predictor variables with centering and scaling. I'll also be using Lasso regularization during training by fixing the `alpha` parameter at 1 ([source link](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html)). Lasso regularization penalizes model complexity by setting the parameter estimates for unimportant variables to zero, effectively acting as a form of feature selection. I will use repeated 10-fold cross validation to select the best value of `lambda` (the regularization parameter). That model will be the selected candidate model for Logistic Regression.

```{r}
# Process training in parallel
# Referenced caret vignette at https://topepo.github.io/caret/parallel-processing.html
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

# Set seed for reproducibility
set.seed(1)

# Do repeated cross validation 3 times
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

# Build the LASSO logistic regression models with different values for lambda
log_reg_fit <- train(sharesBin ~ ., data = df_train, method = "glmnet", family = "binomial",
 trControl=train_control,
 preProcess = c("center", "scale"),
 tuneGrid = expand.grid(.alpha = 1, .lambda = 10^(seq(-5,0, by=0.25))))

# Turn off the cluster for parallel processing
stopCluster(cl)

# Show the results
log_reg_fit
```

~~The article referenced in the UCI website mentions that they made the problem into a binary classification
problem by dividing the shares into two groups (< 1400 and ≥ 1400), you can do this if you’d like or simply
try to predict the shares themselves.~~

~~Feel free to use code similar to the notes or use the caret package.~~

~~After training/tuning your two types of models (linear and non-linear) using cross-validation, AIC, or your
preferred method (all on the training data set only!) you should then compare them on the test set. Your
methodology for choosing your model during the training phase should be explained.~~

### Model Comparison

Now that we have trained a GLM and a non-linear model, let's compare their accuracy on the holdout test data set.

```{r}
get_test_accuracy <- function(model) {
  # Create confusion matrix from predictions
  confusion_matrix <- predict(model, newdata = df_test) %>%
    table(df_test$sharesBin)
  
  # Return accuracy rate
  sum(diag(confusion_matrix))/sum(confusion_matrix)
}

list("Random Forest" = random_forest_fit, "Logistic Regression" = log_reg_fit) %>%
  sapply(get_test_accuracy) %>%
  knitr::kable(caption = "Model Accuracy on Test Set", col.names = "Accuracy")
```


## Automation

Once you’ve completed the above for Monday, adapt the code so that you can use a parameter in your build
process that will cycle through the weekday_is_* variables.



